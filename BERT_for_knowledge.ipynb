{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers\nimport tokenizers\nimport torch\nimport pandas as pd\nimport numpy as np\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"class config:\n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 32\n    VALID_BATCH_SIZE = 16\n    EPOCHS = 5\n    BERT_PATH = \"../input/bert-base-uncased/\"\n    MODEL_PATH = \"model.bin\"\n    TRAINING_FILE = \"../input/tweet-sentiment-extraction/train.csv\"\n    TOKENIZER = tokenizers.BertWordPieceTokenizer(\n        os.path.join(BERT_PATH,\"vocab.txt\"),\n        lowercase=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class TweetDataset:\n    def __init__(self,tweet,sentiment,selected_text):\n        self.tweet=tweet\n        self.sentiment=sentiment\n        self.selected_text=selected_text\n        self.max_len=config.MAX_LEN\n        self.tokenizer=config.TOKENIZER\n        \n    def __len__(self):\n        return len(self.tweet)\n    \n    def __getitem__(self,item):\n        tweet=\" \".join(str(self.tweet[item]).split())\n        selected_text=\" \".join(str(self.selected_text[item]).split())\n        \n        len_sel_text=len(selected_text)\n        idx0=-1\n        idx1=-1\n        for ind in (i for i,e in enumerate(tweet) if e==selected_text[0]):\n            if tweet[ind:ind+len(selected_text)]==selected_text:\n                idx0=ind\n                idx1=ind+len(selected_text)-1\n                break\n        \n        char_targets=[0]*len(tweet)\n        if idx0!=-1 and idx1!=-1:\n            for j in range(idx0,idx1+1):\n                if tweet[j]!=\" \":\n                    char_targets[j]=1\n        tok_tweet=self.tokenizer.encode(tweet)\n        \n        tok_tweet_tokens=tok_tweet.tokens\n        tok_tweet_ids=tok_tweet.ids\n        tok_tweet_offsets=tok_tweet.offsets[1:-1]\n        \n        targets=[0]*(len(tok_tweet_tokens)-2)\n        for j,(offset1,offset2) in enumerate(tok_tweet_offsets):\n            if sum(char_targets[offset1:offset2])>0:\n                targets[j]=1\n        targets=[0]+targets+[0]\n        target_start=[0]*len(targets)\n        target_end=[0]*len(targets)\n        \n        non_zero=np.nonzero(targets)[0]\n        if len(non_zero)>0:\n            target_start[non_zero[0]]=1\n            target_end[non_zero[-1]]=1\n            \n        mask=[1]*len(tok_tweet_ids)\n        token_type_ids=[1]*len(tok_tweet_ids)\n        \n        padding_len=self.max_len-len(tok_tweet_ids)\n        \n        ids=tok_tweet_ids+[0]*padding_len\n        mask=mask+[0]*padding_len\n        token_tpye_ids=token_type_ids+[0]*padding_len\n        targets=targets+[0]*padding_len\n        target_start=target_start+[0]*padding_len\n        target_end=target_end+[0]*padding_len   \n        \n        sentiment=[1,0,0]\n        if self.sentiment[item]==\"positive\":\n            sentiment=[0,0,1]\n        if self.sentiment[item]==\"negative\":\n            sentiment=[0,1,0]\n        return {\n            \"ids\":torch.tensor(ids,dtype=torch.long),\n            \"mask\":torch.tensor(mask,dtype=torch.long),\n            \"token_type_ids\":torch.tensor(token_type_ids,dtype=torch.long),\n            \"targets\":torch.tensor(targets,dtype=torch.long),\n            \"target_start\":torch.tensor(target_start,dtype=torch.long),\n            \"target_end\":torch.tensor(target_end,dtype=torch.long),\n            \"padding_len\":torch.tensor(padding_len,dtype=torch.long),\n            \"tweet_token\":\" \".join(tok_tweet_tokens),\n            \"orig_tweet\":self.tweet[item],\n            \"sentiment\":torch.tensor(sentiment,dtype=torch.long),\n            \"orig_sentiment\":self.sentiment[item],\n            \"orig_selected_text\":self.selected_text[item]\n        }\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__==\"__main__\":\n    df=pd.read_csv(config.TRAINING_FILE).dropna().reset_index(drop=True)\n    dset=TweetDataset(\n        tweet=df.text.values,\n        sentiment=df.sentiment.values,\n        selected_text=df.selected_text.values\n    )\n    print(dset[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch\n\nclass BERTBaseUncased(nn.Module):\n    def __init__(self):\n        super(BERTBaseUncased, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH)\n        self.l0 = nn.Linear(768, 2)\n\n    def forward(self, ids, mask, token_type_ids):\n        sequence_output, pooled_output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n        logits=self.l0(sequence_output)\n        start_logits,end_logits=logits.split(1,dim=-1)\n        start_logits=start_logits.squeeze(1)\n        end_logits=end_logits.squeeze(1)\n        return start_logits,end_logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(o1,o2,t1,t2):\n    l1=nn.BCEWithLogitsLoss(o1,t1)\n    l2=nn.BCEWithLogitsLoss(o2,t2)\n    return l1+l2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AverageMeter():\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(data_loader, model, optimizer, device, scheduler):\n    model.train()\n    losses=AverageMeter()\n    tk0=tqdm(data_loader, total=len(data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        target_start = d[\"target_start\"]\n        target_end = d[\"target_end\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        target_start = target_start.to(device, dtype=torch.float)\n        target_end = target_end.to(device, dtype=torch.float)\n\n        optimizer.zero_grad()\n        o1,o2 = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n\n        loss = loss_fn(o1,o2,target_start,target_end)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        losses.update(loss.item(),ids.size(0))\n        tk0.set_postfix(loss=losses.avg)           \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom tqdm import tqdm\n\ndef run():\n    dfx = pd.read_csv(config.TRAINING_FILE).dropna().reset_index(drop=True)\n\n    df_train, df_valid = model_selection.train_test_split(\n        dfx, \n        test_size=0.1, \n        random_state=42, \n        stratify=dfx.sentiment.values\n    )\n\n    df_train = df_train.reset_index(drop=True)\n    df_valid = df_valid.reset_index(drop=True)\n\n    train_dataset = TweetDataset(\n        tweet=df_train.text.values, \n        sentiment=df_train.sentiment.values,\n        selected_text=df_train.selected_text.values\n    )\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=config.TRAIN_BATCH_SIZE, num_workers=4\n    )\n\n    valid_dataset = TweetDataset(\n        tweet=df_valid.text.values, \n        sentiment=df_valid.sentiment.values,\n        selected_text=df_valid.selected_text.values\n    )\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset, batch_size=config.VALID_BATCH_SIZE, num_workers=1\n    )\n\n    device = torch.device(\"cuda\")\n    model = BERTBaseUncased()\n    model.to(device)\n\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {\n            \"params\": [\n                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.001,\n        },\n        {\n            \"params\": [\n                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.0,\n        },\n    ]\n\n    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n    )\n\n    model = nn.DataParallel(model)\n\n    best_jaccord = 0\n    for epoch in range(config.EPOCHS):\n        train_fn(train_data_loader, model, optimizer, device, scheduler)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nif __name__ == \"__main__\":\n    run()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}